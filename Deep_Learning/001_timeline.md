# Neural Network Timeline
- **1948** -   [McCulloch-Pitts neurons](http://www.mind.ilstu.edu/curriculum/modOverview.php?modGUI=212) First Mathematical Model of a NN.

	 Walter Pitts, a logician, and Warren McCulloch, a neuroscientist, gave us that piece of the puzzle in 1943 when they created the first mathematical model of a neural network. Published in their seminal work “[A Logical Calculus of Ideas Immanent in Nervous Activity](https://dl.acm.org/citation.cfm?id=104377)”, they proposed a combination of mathematics and algorithms that aimed to mimic human thought processes.

- **1965** – The first working deep learning networks 
	
	Mathematician Ivakhnenko and associates including Lapa arguably created the first _working_ deep learning networks in 1965. For that reason alone, many consider *Ivakhnenko the father of modern deep learning*

- **1979** - The First Convolutional Neural Network

	A recognized innovator in neural networks, Fukushima is perhaps best known for the creation of [Neocognitron](http://www.scholarpedia.org/article/Neocognitron), an artificial neural network that learned how to recognize visual patterns. It has been used for handwritten character and other pattern recognition tasks, recommender systems, and even natural language processing

- **1982** - Hopefield Network
	
	In 1982, Hopfield created and popularized the system that now bears his name.
	[Hopfield Networks](http://www.scholarpedia.org/article/Hopfield_network) are a [recurrent neural network](https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82) that serve as a content-addressable memory system, and they remain a popular implementation tool for deep learning in the 21st century.
	
	
- **1986** – Backpropagation -  Improvements in shape recognition and word prediction

	In a 1986 paper entitled “Learning Representations by Back-propagating Errors,” Rumelhart, Hinton, and Williams described in greater detail the process of backpropagation. 
	
	*Geoffrey Hinton is considered by many in the field to be the godfather of deep learning*
	
- **1989** -  Machines read handwritten digits ( CNN + BP)

	Yann LeCun – another rock star in the AI and DL universe – combined Convolutional neural networks (which he was instrumental in developing) with recent backpropagation theories to read handwritten digits in 1989.

- **1989** - Q-Learning (RL)

	Watkins published his PhD thesis – “[Learning from Delayed Rewards](https://www.import.io/wp-content/uploads/2017/06/new_thesis.pdf)” – in 1989. In it, he introduced the concept of [Q-learning](https://en.wikipedia.org/wiki/Q-learning), which greatly improves the practicality _and_ feasibility of [reinforcement learning](https://deepmind.com/blog/deep-reinforcement-learning/) in machines
	
- **1997** - LSTM

	A recurrent neural network framework, [long short-term memory (LSTM)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) was proposed by Schmidhuber and Hochreiter in 1997.

- **1998** Gradient Based Learning

	LeCun was instrumental in yet another advancement in the field of deep learning when he published his “[Gradient-Based Learning Applied to Document Recognition](https://www.import.io/wp-content/uploads/2017/06/lecun-01a.pdf)” paper in 1998.The [Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Applications) algorithm (aka gradient-based learning) combined with the [backpropagation](https://brilliant.org/wiki/backpropagation/) algorithm is the preferred and increasingly successful approach to deep learning.

- **2014** - Generative Adversarial Networks (GAN)

	Introduced in 2014 by a team of researchers lead by Ian Goodfellow.



## Rough Timeline
-   1960s: Shallow neural networks
-   1960-70s: Backpropagation emerges
-   1974-80: First [AI Winter](https://en.wikipedia.org/wiki/AI_winter)
-   1980s: Convolution emerges
-   1987-93: Second AI Winter
-   1990s: Unsupervised deep learning
-   1990s-2000s: Supervised deep learning back en vogue
-   2006s-present: Modern deep learning

---
Related pages : [[Deep Learning/000_Overview]]
#machinelearning  #history #deeplearning 